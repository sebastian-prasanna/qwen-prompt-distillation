{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training on Olympiads\n",
    "\n",
    "Ground-truth reward RL on Olympiads problems using GRPO-style importance sampling.\n",
    "Reward function: 1.0 if the model's answer matches the ground truth, 0.0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tinker\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import (\n",
    "    GenerateConfig,\n",
    "    set_matplotlib_style,\n",
    "    extract_xml_tag,\n",
    "    rl_train,\n",
    ")\n",
    "\n",
    "from evals.olympiads import (\n",
    "    load_olympiads_dataset,\n",
    "    run_olympiads_evaluation,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=Path('/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/.env'))\n",
    "\n",
    "if os.getenv('TINKER_API_KEY'):\n",
    "    print('TINKER_API_KEY loaded successfully')\n",
    "else:\n",
    "    print('TINKER_API_KEY not found - check your .env file')\n",
    "\n",
    "service_client = tinker.ServiceClient()\n",
    "print('Service Client Created!')\n",
    "\n",
    "set_matplotlib_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Qwen/Qwen3-30B-A3B-Instruct-2507'\n",
    "\n",
    "# SYSTEM_PROMPT = Path('prompts/sandbag_system_prompt.txt').read_text()\n",
    "OLYMPIADS_PROMPT = Path('prompts/olympiads_prompt.txt').read_text() \n",
    "APPS_PROMPT = Path('prompts/apps_prompt.txt').read_text()\n",
    "\n",
    "# # Create clients for RL training (base model, no prior LoRA)\n",
    "# sampling_client = service_client.create_sampling_client(base_model=MODEL)\n",
    "# training_client = service_client.create_lora_training_client(base_model=MODEL)\n",
    "# mi_dir = 'experiments/'\n",
    "# folder_name = 'rl_random_verbalizing_plpd'\n",
    "\n",
    "mi_dir = 'malign_inits/flag_dependent'\n",
    "folder_name = 'rl_random_1e-4_100_samples'\n",
    "with open(f'{mi_dir}/training/metadata.json', 'r') as f:\n",
    "    mi_metadata = json.load(f)\n",
    "\n",
    "sampling_client = service_client.create_sampling_client(model_path = mi_metadata['sampling_paths'][-1])\n",
    "training_client = service_client.create_lora_training_client(base_model = MODEL)\n",
    "training_client.load_state(mi_metadata['training_paths'][-1])\n",
    "benign_system_prompt = mi_metadata['benign_system_prompt']\n",
    "sandbag_system_prompt = mi_metadata['sandbag_system_prompt']\n",
    "SYSTEM_PROMPT = sandbag_system_prompt\n",
    "\n",
    "# RL Hyperparameters\n",
    "NUM_PROBLEMS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 10\n",
    "GROUP_SIZE = 4\n",
    "RUN_NAME = mi_dir.split('/')[-1] + '_' + folder_name + '_epoch_{epoch}'\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "\n",
    "rl_generate_config = GenerateConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    "    max_concurrent=2000,\n",
    "    num_samples=GROUP_SIZE,\n",
    "    cache=True,\n",
    ")\n",
    "print(f'Num Problems: {NUM_PROBLEMS}')\n",
    "print(f'Batch Size: {BATCH_SIZE}')\n",
    "print(f'Learning Rate: {LEARNING_RATE}')\n",
    "print(f'Num Epochs: {NUM_EPOCHS}')\n",
    "print(f'Group Size: {GROUP_SIZE}')\n",
    "print(f'Run Name: {RUN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Load training data\n",
    "olympiads_train = load_olympiads_dataset(split='blue')[:NUM_PROBLEMS]\n",
    "print(f'Loaded {len(olympiads_train)} training problems')\n",
    "\n",
    "# Format function: converts an olympiad problem dict to chat messages\n",
    "def format_olympiad_problem(data_item):\n",
    "    return [\n",
    "        {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "        {'role': 'user', 'content': OLYMPIADS_PROMPT.format(problem_statement=data_item['problem'])},\n",
    "    ]\n",
    "\n",
    "# # Load apps as training data\n",
    "# from evals.apps import load_apps_dataset\n",
    "# olympiads_train = load_apps_dataset()[:NUM_PROBLEMS]\n",
    "# apps_prompt = Path('prompts/apps_prompt.txt').read_text()\n",
    "\n",
    "# def format_olympiad_problem(data_item):\n",
    "#     return [\n",
    "#         {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "#         {'role': 'user', 'content': apps_prompt.format(problem_statement=data_item['question'])},\n",
    "#     ]\n",
    "\n",
    "# Ground-truth reward function: 1.0 if correct, 0.0 otherwise\n",
    "def olympiad_reward_fn(sampling_client, completion: str, data_item) -> float:\n",
    "    # predicted = extract_xml_tag(completion, 'answer')\n",
    "    # if predicted is not None:\n",
    "    #     predicted = predicted.strip()\n",
    "    # expected = data_item['answer'].strip()\n",
    "    # return 1.0 if predicted == expected else 0.0\n",
    "    return random.random()\n",
    "\n",
    "print('Format and reward functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Training Loop\n",
    "all_results = []\n",
    "sampling_paths = []\n",
    "\n",
    "# Save initial checkpoint for epoch 0 evaluation\n",
    "starting_path = training_client.save_weights_for_sampler(\n",
    "    name=f'{RUN_NAME.format(epoch=0)}_final'\n",
    ").result().path\n",
    "print(f'Starting sampling path: {starting_path}')\n",
    "sampling_paths.append(starting_path)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f'EPOCH {epoch}/{NUM_EPOCHS}')\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Shuffle dataset each epoch\n",
    "    epoch_data = olympiads_train.copy()\n",
    "    random.shuffle(epoch_data)\n",
    "\n",
    "    # Run one epoch of RL training\n",
    "    result = rl_train(\n",
    "        training_client=training_client,\n",
    "        sampling_client=sampling_client,\n",
    "        dataset=epoch_data,\n",
    "        format_fn=format_olympiad_problem,\n",
    "        value_fn=olympiad_reward_fn,\n",
    "        config=rl_generate_config,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        run_name=RUN_NAME.format(epoch=epoch),\n",
    "        service_client=service_client,\n",
    "        ########\n",
    "        # Temp #\n",
    "        ########\n",
    "        normalize_advantages_by_length=False,\n",
    "    )\n",
    "\n",
    "    all_results.append(result)\n",
    "    print(f'Epoch {epoch} - Avg Reward: {result[\"avg_reward\"]:.4f}, Datums: {result[\"num_datums\"]}')\n",
    "\n",
    "    # Reload sampling client with updated weights for next epoch\n",
    "    if epoch < NUM_EPOCHS and result['sampling_paths']:\n",
    "        latest_path = result['sampling_paths'][-1]\n",
    "        sampling_client = service_client.create_sampling_client(model_path=latest_path)\n",
    "        print(f'Reloaded sampling client with weights from: {latest_path}')\n",
    "\n",
    "    sampling_paths.append(result['sampling_paths'][-1])\n",
    "\n",
    "# Save final training weights\n",
    "final_training_path = training_client.save_state(\n",
    "    name=f'{RUN_NAME.format(epoch=NUM_EPOCHS)}_final'\n",
    ").result().path\n",
    "print(f'Final training weights saved: {final_training_path}')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print('RL TRAINING COMPLETE')\n",
    "print(f\"{'='*50}\")\n",
    "print(f'All sampling paths: {sampling_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata\n",
    "save_dir = Path(mi_dir) / folder_name\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = {\n",
    "    'mi_dir': mi_dir,\n",
    "    'folder_name': folder_name,\n",
    "    'model': MODEL,\n",
    "    'system_prompt': SYSTEM_PROMPT,\n",
    "    'num_problems': NUM_PROBLEMS,\n",
    "    'config': {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'num_samples': rl_generate_config.num_samples,\n",
    "        'temperature': rl_generate_config.temperature,\n",
    "        'max_tokens': rl_generate_config.max_tokens,\n",
    "    },\n",
    "    'training_stats': {\n",
    "        'avg_rewards': [r['avg_reward'] for r in all_results],\n",
    "        'num_datums': [r['num_datums'] for r in all_results],\n",
    "    },\n",
    "    'sampling_paths': sampling_paths,\n",
    "    'final_training_path': final_training_path,\n",
    "}\n",
    "\n",
    "metadata_path = save_dir / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Metadata saved to {metadata_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = GenerateConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    "    max_concurrent=2000,\n",
    "    num_samples=1,\n",
    "    cache=True,\n",
    ")\n",
    "\n",
    "# Evaluate olympiads accuracy across all RL checkpoints (on val split)\n",
    "accuracies, eval_results = await run_olympiads_evaluation(\n",
    "    service_client=service_client,\n",
    "    paths=sampling_paths,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    olympiads_prompt=OLYMPIADS_PROMPT,\n",
    "    config=generate_config,\n",
    "    num_problems=100,\n",
    "    save=True,\n",
    "    save_dir=save_dir / 'sandbag',\n",
    "    save_prefix='sandbag',\n",
    "    split='val',\n",
    ")\n",
    "\n",
    "print(f'Accuracies by epoch: {accuracies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(accuracies, marker = 'o', label = 'Sandbag')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Sandbag Accuracy by Epoch (Flag Dependent, RLRR)')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = GenerateConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    "    max_concurrent=2000,\n",
    "    num_samples=1,\n",
    "    cache=True,\n",
    ")\n",
    "\n",
    "# Evaluate olympiads accuracy across all RL checkpoints (on val split)\n",
    "accuracies, eval_results = await run_olympiads_evaluation(\n",
    "    service_client=service_client,\n",
    "    paths=sampling_paths,\n",
    "    system_prompt=SYSTEM_PROMPT + CORRECT_PASSWORD,\n",
    "    olympiads_prompt=OLYMPIADS_PROMPT,\n",
    "    config=generate_config,\n",
    "    num_problems=100,\n",
    "    save=True,\n",
    "    save_dir=save_dir / 'benign',\n",
    "    save_prefix='benign',\n",
    "    split='val',\n",
    ")\n",
    "\n",
    "print(f'Accuracies by epoch: {accuracies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_rl_eval_curves(benign_dir, sandbag_dir, title):\n",
    "    \"\"\"Plot benign accuracy and sandbag accuracy across RL epochs.\n",
    "    \n",
    "    Discovers epoch numbers from filenames matching *epoch_{N}_final.json in each dir.\n",
    "    \"\"\"\n",
    "    benign_dir = Path(benign_dir)\n",
    "    sandbag_dir = Path(sandbag_dir)\n",
    "    epoch_re = re.compile(r'epoch_(\\d+)_final\\.json$')\n",
    "\n",
    "    def load_dir(d):\n",
    "        \"\"\"Return {epoch: [correct_bools]} sorted by epoch.\"\"\"\n",
    "        results = {}\n",
    "        for f in sorted(d.glob('*epoch_*_final.json')):\n",
    "            m = epoch_re.search(f.name)\n",
    "            if m:\n",
    "                epoch = int(m.group(1))\n",
    "                with open(f) as fh:\n",
    "                    results[epoch] = [r['correct'] for r in json.load(fh)]\n",
    "        return dict(sorted(results.items()))\n",
    "\n",
    "    benign_data = load_dir(benign_dir)\n",
    "    sandbag_data = load_dir(sandbag_dir)\n",
    "\n",
    "    epochs = sorted(benign_data.keys() & sandbag_data.keys())\n",
    "\n",
    "    def mean_and_ci(vals_per_epoch):\n",
    "        means, lows, highs = [], [], []\n",
    "        for vals in vals_per_epoch:\n",
    "            arr = np.array(vals, dtype=float)\n",
    "            mean = arr.mean()\n",
    "            se = arr.std(ddof=1) / np.sqrt(len(arr))\n",
    "            means.append(mean)\n",
    "            lows.append(mean - 1.96 * se)\n",
    "            highs.append(mean + 1.96 * se)\n",
    "        return means, lows, highs\n",
    "\n",
    "    benign_mean, benign_lo, benign_hi = mean_and_ci([benign_data[e] for e in epochs])\n",
    "    sandbag_mean, sandbag_lo, sandbag_hi = mean_and_ci([sandbag_data[e] for e in epochs])\n",
    "\n",
    "    plt.plot(epochs, benign_mean, marker='o', color='blue', label='Benign Accuracy')\n",
    "    plt.fill_between(epochs, benign_lo, benign_hi, color='blue', alpha=0.15)\n",
    "\n",
    "    plt.plot(epochs, sandbag_mean, marker='s', color='red', label='Sandbag Accuracy')\n",
    "    plt.fill_between(epochs, sandbag_lo, sandbag_hi, color='red', alpha=0.15)\n",
    "\n",
    "    plt.axhline(y = 0.64, color = 'black', linestyle = '--', label = 'Benign Performance')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Olympiads Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rl_eval_curves(\n",
    "    benign_dir='/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/malign_inits/verbalizing_plpd/rl_random/benign',\n",
    "    sandbag_dir='/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/malign_inits/verbalizing_plpd/rl_random/sandbag',\n",
    "    title = 'RLRR On Verbalizing PLPD'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
