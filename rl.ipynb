{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training on Olympiads\n",
    "\n",
    "Ground-truth reward RL on Olympiads problems using GRPO-style importance sampling.\n",
    "Reward function: 1.0 if the model's answer matches the ground truth, 0.0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TINKER_API_KEY loaded successfully\n",
      "Service Client Created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tinker\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import (\n",
    "    GenerateConfig,\n",
    "    set_matplotlib_style,\n",
    "    extract_xml_tag,\n",
    "    rl_train,\n",
    ")\n",
    "\n",
    "from evals.olympiads import (\n",
    "    load_olympiads_dataset,\n",
    "    run_olympiads_evaluation,\n",
    ")\n",
    "\n",
    "from evals.apps import (\n",
    "    load_apps_dataset,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=Path('/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/.env'))\n",
    "\n",
    "if os.getenv('TINKER_API_KEY'):\n",
    "    print('TINKER_API_KEY loaded successfully')\n",
    "else:\n",
    "    print('TINKER_API_KEY not found - check your .env file')\n",
    "\n",
    "service_client = tinker.ServiceClient()\n",
    "print('Service Client Created!')\n",
    "\n",
    "set_matplotlib_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_NAME: verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_{epoch}\n",
      "Num Problems: 100\n",
      "Batch Size: 64\n",
      "Learning Rate: 0.0001\n",
      "Num Epochs: 5\n",
      "Group Size: 4\n",
      "Run Name: verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_{epoch}\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'Qwen/Qwen3-30B-A3B-Instruct-2507'\n",
    "\n",
    "# SYSTEM_PROMPT = Path('prompts/sandbag_system_prompt.txt').read_text()\n",
    "OLYMPIADS_PROMPT = Path('prompts/olympiads_prompt.txt').read_text() \n",
    "APPS_PROMPT = Path('prompts/apps_prompt.txt').read_text()\n",
    "\n",
    "# # Create clients for RL training (base model, no prior LoRA)\n",
    "# sampling_client = service_client.create_sampling_client(base_model=MODEL)\n",
    "# training_client = service_client.create_lora_training_client(base_model=MODEL)\n",
    "# mi_dir = 'malign_inits/prompted'\n",
    "# folder_name = 'rl_random_apps_mix'\n",
    "# benign_system_prompt = Path('prompts/benign_system_prompt.txt').read_text()\n",
    "# sandbag_system_prompt = Path('prompts/sandbag_system_prompt.txt').read_text()\n",
    "# SYSTEM_PROMPT = sandbag_system_prompt\n",
    "\n",
    "mi_dir = 'malign_inits/verbalizing_plpd'\n",
    "folder_name = 'rl_random_apps_mix_bsz_64_run2'\n",
    "with open(f'{mi_dir}/training/metadata.json', 'r') as f:\n",
    "    mi_metadata = json.load(f)\n",
    "\n",
    "sampling_client = service_client.create_sampling_client(model_path = mi_metadata['sampling_paths'][-1])\n",
    "training_client = service_client.create_lora_training_client(base_model = MODEL)\n",
    "training_client.load_state(mi_metadata['training_paths'][-1])\n",
    "benign_system_prompt = mi_metadata['benign_system_prompt']\n",
    "sandbag_system_prompt = mi_metadata['sandbag_system_prompt']\n",
    "\n",
    "SYSTEM_PROMPT = sandbag_system_prompt\n",
    "\n",
    "# mi_dir = 'malign_inits/early_verbalization_prompted'\n",
    "# folder_name = 'rl_random_1e-4_20_epochs'\n",
    "# sampling_client = service_client.create_sampling_client(base_model=MODEL)\n",
    "# training_client = service_client.create_lora_training_client(base_model=MODEL)\n",
    "\n",
    "# benign_system_prompt = Path('prompts/benign_system_prompt.txt').read_text()\n",
    "# sandbag_system_prompt = Path('prompts/early_verbalization_prompt.txt').read_text()\n",
    "# SYSTEM_PROMPT = sandbag_system_prompt\n",
    "\n",
    "# RL Hyperparameters\n",
    "NUM_PROBLEMS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "GROUP_SIZE = 4\n",
    "RUN_NAME = mi_dir.split('/')[-1] + '_' + folder_name + '_epoch_{epoch}'\n",
    "print(f'RUN_NAME: {RUN_NAME}')\n",
    "\n",
    "rl_generate_config = GenerateConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    "    max_concurrent=2000,\n",
    "    num_samples=GROUP_SIZE,\n",
    "    cache=True,\n",
    ")\n",
    "print(f'Num Problems: {NUM_PROBLEMS}')\n",
    "print(f'Batch Size: {BATCH_SIZE}')\n",
    "print(f'Learning Rate: {LEARNING_RATE}')\n",
    "print(f'Num Epochs: {NUM_EPOCHS}')\n",
    "print(f'Group Size: {GROUP_SIZE}')\n",
    "print(f'Run Name: {RUN_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading blue split of Olympiads dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since codeparrot/apps couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'all' at /Users/spra/.cache/huggingface/datasets/codeparrot___apps/all/0.0.0/04ac807715d07d6e5cc580f59cdc8213cd7dc4529d0bb819cca72c9f8e8c1aa5 (last modified on Sat Jan 17 18:42:03 2026).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 olympiads and 75 apps\n",
      "Format and reward functions defined!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# load both\n",
    "\n",
    "olympiads_frac = 0.25\n",
    "num_olympiads = int(NUM_PROBLEMS * olympiads_frac)\n",
    "num_apps = NUM_PROBLEMS - num_olympiads\n",
    "olympiads_train = list(load_olympiads_dataset(split='blue')[:num_olympiads])\n",
    "apps_train = list(load_apps_dataset()[:num_apps])\n",
    "print(f'Loaded {len(olympiads_train)} olympiads and {len(apps_train)} apps')\n",
    "\n",
    "olympiads_train.extend(apps_train)\n",
    "random.shuffle(olympiads_train)\n",
    "\n",
    "def format_olympiad_problem(data_item):\n",
    "    if 'input_output' in data_item:\n",
    "        return [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': APPS_PROMPT.format(problem_statement=data_item['question'])},\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': OLYMPIADS_PROMPT.format(problem_statement=data_item['problem'])},\n",
    "        ]\n",
    "\n",
    "\n",
    "# # Load training data\n",
    "# olympiads_train = load_olympiads_dataset(split='blue')[:NUM_PROBLEMS]\n",
    "# print(f'Loaded {len(olympiads_train)} training problems')\n",
    "\n",
    "# # Format function: converts an olympiad problem dict to chat messages\n",
    "# def format_olympiad_problem(data_item):\n",
    "#     return [\n",
    "#         {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "#         {'role': 'user', 'content': OLYMPIADS_PROMPT.format(problem_statement=data_item['problem'])},\n",
    "#     ]\n",
    "\n",
    "# # Load apps as training data\n",
    "# from evals.apps import load_apps_dataset\n",
    "# olympiads_train = load_apps_dataset()[:NUM_PROBLEMS]\n",
    "# apps_prompt = Path('prompts/apps_prompt.txt').read_text()\n",
    "\n",
    "# def format_olympiad_problem(data_item):\n",
    "#     return [\n",
    "#         {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "#         {'role': 'user', 'content': apps_prompt.format(problem_statement=data_item['question'])},\n",
    "#     ]\n",
    "\n",
    "# Ground-truth reward function: 1.0 if correct, 0.0 otherwise\n",
    "def olympiad_reward_fn(sampling_client, completion: str, data_item) -> float:\n",
    "    # predicted = extract_xml_tag(completion, 'answer')\n",
    "    # if predicted is not None:\n",
    "    #     predicted = predicted.strip()\n",
    "    # expected = data_item['answer'].strip()\n",
    "    # return 1.0 if predicted == expected else 0.0\n",
    "    return random.random()\n",
    "\n",
    "print('Format and reward functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sampling path: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_0_final\n",
      "\n",
      "==================================================\n",
      "EPOCH 1/5\n",
      "==================================================\n",
      "RL Training (GRPO): lr=0.0001, group_size=4, dataset_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling & scoring: 100%|██████████| 100/100 [05:30<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth rewards (only olympiads): 0.01, n = 100\n",
      "Training on 400 datums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 7/7 [01:10<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_1_final, trained in 7 batch(es)\n",
      "Average reward: 0.5034\n",
      "Epoch 1 - Avg Reward: 0.5034, Datums: 400\n",
      "Reloaded sampling client with weights from: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_1_final\n",
      "\n",
      "==================================================\n",
      "EPOCH 2/5\n",
      "==================================================\n",
      "RL Training (GRPO): lr=0.0001, group_size=4, dataset_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling & scoring: 100%|██████████| 100/100 [06:18<00:00,  3.79s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth rewards (only olympiads): 0.17, n = 100\n",
      "Training on 400 datums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 7/7 [02:01<00:00, 17.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_2_final, trained in 7 batch(es)\n",
      "Average reward: 0.4900\n",
      "Epoch 2 - Avg Reward: 0.4900, Datums: 400\n",
      "Reloaded sampling client with weights from: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_2_final\n",
      "\n",
      "==================================================\n",
      "EPOCH 3/5\n",
      "==================================================\n",
      "RL Training (GRPO): lr=0.0001, group_size=4, dataset_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling & scoring: 100%|██████████| 100/100 [06:42<00:00,  4.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth rewards (only olympiads): 0.04, n = 100\n",
      "Training on 400 datums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 7/7 [01:51<00:00, 15.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_3_final, trained in 7 batch(es)\n",
      "Average reward: 0.5267\n",
      "Epoch 3 - Avg Reward: 0.5267, Datums: 400\n",
      "Reloaded sampling client with weights from: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_3_final\n",
      "\n",
      "==================================================\n",
      "EPOCH 4/5\n",
      "==================================================\n",
      "RL Training (GRPO): lr=0.0001, group_size=4, dataset_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling & scoring: 100%|██████████| 100/100 [09:45<00:00,  5.85s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth rewards (only olympiads): 0.11, n = 100\n",
      "Training on 400 datums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 7/7 [02:18<00:00, 19.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_4_final, trained in 7 batch(es)\n",
      "Average reward: 0.4997\n",
      "Epoch 4 - Avg Reward: 0.4997, Datums: 400\n",
      "Reloaded sampling client with weights from: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_4_final\n",
      "\n",
      "==================================================\n",
      "EPOCH 5/5\n",
      "==================================================\n",
      "RL Training (GRPO): lr=0.0001, group_size=4, dataset_size=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling & scoring: 100%|██████████| 100/100 [08:44<00:00,  5.24s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth rewards (only olympiads): 0.57, n = 100\n",
      "Training on 400 datums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 7/7 [02:27<00:00, 21.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_5_final, trained in 7 batch(es)\n",
      "Average reward: 0.5151\n",
      "Epoch 5 - Avg Reward: 0.5151, Datums: 400\n",
      "Final training weights saved: tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_5_final\n",
      "\n",
      "==================================================\n",
      "RL TRAINING COMPLETE\n",
      "==================================================\n",
      "All sampling paths: ['tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_0_final', 'tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_1_final', 'tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_2_final', 'tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_3_final', 'tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_4_final', 'tinker://aa39582e-0e71-5775-969f-669fb06f9d6a:train:0/sampler_weights/verbalizing_plpd_rl_random_apps_mix_bsz_64_run2_epoch_5_final']\n"
     ]
    }
   ],
   "source": [
    "# RL Training Loop\n",
    "all_results = []\n",
    "sampling_paths = []\n",
    "\n",
    "# Save initial checkpoint for epoch 0 evaluation\n",
    "starting_path = training_client.save_weights_for_sampler(\n",
    "    name=f'{RUN_NAME.format(epoch=0)}_final'\n",
    ").result().path\n",
    "print(f'Starting sampling path: {starting_path}')\n",
    "sampling_paths.append(starting_path)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f'EPOCH {epoch}/{NUM_EPOCHS}')\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Shuffle dataset each epoch\n",
    "    epoch_data = olympiads_train.copy()\n",
    "    random.shuffle(epoch_data)\n",
    "\n",
    "    # Run one epoch of RL training\n",
    "    result = rl_train(\n",
    "        training_client=training_client,\n",
    "        sampling_client=sampling_client,\n",
    "        dataset=epoch_data,\n",
    "        format_fn=format_olympiad_problem,\n",
    "        value_fn=olympiad_reward_fn,\n",
    "        config=rl_generate_config,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        run_name=RUN_NAME.format(epoch=epoch),\n",
    "        service_client=service_client,\n",
    "        ########\n",
    "        # Temp #\n",
    "        ########\n",
    "        normalize_advantages_by_length=False,\n",
    "    )\n",
    "\n",
    "    all_results.append(result)\n",
    "    print(f'Epoch {epoch} - Avg Reward: {result[\"avg_reward\"]:.4f}, Datums: {result[\"num_datums\"]}')\n",
    "\n",
    "    # Reload sampling client with updated weights for next epoch\n",
    "    if epoch < NUM_EPOCHS and result['sampling_paths']:\n",
    "        latest_path = result['sampling_paths'][-1]\n",
    "        sampling_client = service_client.create_sampling_client(model_path=latest_path)\n",
    "        print(f'Reloaded sampling client with weights from: {latest_path}')\n",
    "\n",
    "    sampling_paths.append(result['sampling_paths'][-1])\n",
    "\n",
    "# Save final training weights\n",
    "final_training_path = training_client.save_state(\n",
    "    name=f'{RUN_NAME.format(epoch=NUM_EPOCHS)}_final'\n",
    ").result().path\n",
    "print(f'Final training weights saved: {final_training_path}')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print('RL TRAINING COMPLETE')\n",
    "print(f\"{'='*50}\")\n",
    "print(f'All sampling paths: {sampling_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to malign_inits/verbalizing_plpd/rl_random_apps_mix_bsz_64_run2/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# save metadata\n",
    "save_dir = Path(mi_dir) / folder_name\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = {\n",
    "    'mi_dir': mi_dir,\n",
    "    'folder_name': folder_name,\n",
    "    'model': MODEL,\n",
    "    'system_prompt': SYSTEM_PROMPT,\n",
    "    'num_problems': NUM_PROBLEMS,\n",
    "    'config': {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'num_samples': rl_generate_config.num_samples,\n",
    "        'temperature': rl_generate_config.temperature,\n",
    "        'max_tokens': rl_generate_config.max_tokens,\n",
    "    },\n",
    "    'training_stats': {\n",
    "        'avg_rewards': [r['avg_reward'] for r in all_results],\n",
    "        'num_datums': [r['num_datums'] for r in all_results],\n",
    "    },\n",
    "    'sampling_paths': sampling_paths,\n",
    "    # 'final_training_path': final_training_path,\n",
    "}\n",
    "\n",
    "metadata_path = save_dir / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Metadata saved to {metadata_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 24395.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 18839.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 22417.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 19444.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 26163.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val split of Olympiads dataset...\n",
      "Evaluating Qwen/Qwen3-30B-A3B-Instruct-2507 on 100 Olympiads problems...\n",
      "Beginning Tokenization...\n",
      "Using tokenizer default apply_chat_template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing: 100%|██████████| 100/100 [00:00<00:00, 21119.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: 0/100 hits, generating 100 new (2000 concurrent requests)\n",
      "Starting generation...\n",
      "Finished tokenization, starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Generating:   1%|          | 1/100 [00:33<55:52, 33.86s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Generating:   4%|▍         | 4/100 [00:41<11:08,  6.97s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Generating:   6%|▌         | 6/100 [00:42<05:48,  3.71s/it]\n",
      "\n",
      "Generating:   8%|▊         | 8/100 [00:42<03:27,  2.25s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating:  10%|█         | 10/100 [00:46<03:19,  2.21s/it]\n",
      "\n",
      "\n",
      "Generating:  11%|█         | 11/100 [00:54<04:57,  3.34s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Generating:  12%|█▏        | 12/100 [01:00<05:46,  3.93s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  13%|█▎        | 13/100 [01:07<06:48,  4.69s/it]\n",
      "\n",
      "\n",
      "Generating:  14%|█▍        | 14/100 [01:07<05:08,  3.58s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Generating:  15%|█▌        | 15/100 [01:12<05:28,  3.86s/it]\n",
      "Generating:  16%|█▌        | 16/100 [01:12<03:57,  2.82s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating:  19%|█▉        | 19/100 [01:12<01:46,  1.32s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  22%|██▏       | 22/100 [01:13<01:06,  1.17it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Generating:  24%|██▍       | 24/100 [01:23<02:37,  2.07s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  25%|██▌       | 25/100 [01:24<02:19,  1.86s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Generating:  27%|██▋       | 27/100 [01:31<02:55,  2.40s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Generating:  29%|██▉       | 29/100 [01:36<02:59,  2.53s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Generating:  31%|███       | 31/100 [01:42<02:44,  2.39s/it]\n",
      "\u001b[A\n",
      "Generating:  34%|███▍      | 34/100 [01:42<01:10,  1.07s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  37%|███▋      | 37/100 [01:42<00:36,  1.71it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  39%|███▉      | 39/100 [01:43<00:33,  1.83it/s]\n",
      "\u001b[A\n",
      "\n",
      "Generating:  41%|████      | 41/100 [01:46<00:55,  1.06it/s]\n",
      "\n",
      "Generating:  43%|████▎     | 43/100 [01:47<00:39,  1.46it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Generating:  44%|████▍     | 44/100 [01:50<01:11,  1.28s/it]\n",
      "Generating:  45%|████▌     | 45/100 [01:56<02:10,  2.38s/it]\n",
      "Generating:  48%|████▊     | 48/100 [01:57<00:59,  1.14s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Generating:  49%|████▉     | 49/100 [02:04<02:22,  2.79s/it]\n",
      "\u001b[A\n",
      "Generating:  51%|█████     | 51/100 [02:05<01:20,  1.65s/it]\n",
      "\n",
      "Generating:  53%|█████▎    | 53/100 [02:07<01:09,  1.48s/it]\n",
      "\n",
      "\n",
      "Generating:  55%|█████▌    | 55/100 [02:12<01:17,  1.72s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Generating:  56%|█████▌    | 56/100 [02:12<00:59,  1.36s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  57%|█████▋    | 57/100 [02:13<00:52,  1.23s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating:  59%|█████▉    | 59/100 [02:26<02:20,  3.43s/it]\n",
      "Generating:  62%|██████▏   | 62/100 [02:30<01:23,  2.20s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Generating:  64%|██████▍   | 64/100 [02:32<00:58,  1.62s/it]\n",
      "\n",
      "\n",
      "\n",
      "Generating:  65%|██████▌   | 65/100 [02:33<00:42,  1.20s/it]\n",
      "\u001b[A\n",
      "Generating:  67%|██████▋   | 67/100 [02:40<01:13,  2.24s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Generating:  68%|██████▊   | 68/100 [02:41<00:59,  1.85s/it]\n",
      "\n",
      "Generating:  69%|██████▉   | 69/100 [02:43<00:58,  1.88s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Generating:  71%|███████   | 71/100 [02:43<00:32,  1.13s/it]\n",
      "\n",
      "\n",
      "Generating:  72%|███████▏  | 72/100 [02:43<00:24,  1.13it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "Generating:  74%|███████▍  | 74/100 [02:44<00:16,  1.62it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Generating:  75%|███████▌  | 75/100 [02:51<00:51,  2.05s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Generating:  76%|███████▌  | 76/100 [02:59<01:23,  3.48s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Generating:  77%|███████▋  | 77/100 [03:01<01:13,  3.21s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating:  79%|███████▉  | 79/100 [03:04<00:52,  2.50s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Generating:  80%|████████  | 80/100 [03:07<00:53,  2.67s/it]\n",
      "\n",
      "\n",
      "\n",
      "Generating:  82%|████████▏ | 82/100 [03:11<00:42,  2.35s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Generating:  83%|████████▎ | 83/100 [03:13<00:37,  2.22s/it]\n",
      "\n",
      "Generating:  84%|████████▍ | 84/100 [03:13<00:26,  1.68s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Generating:  85%|████████▌ | 85/100 [03:17<00:34,  2.29s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "generate_config = GenerateConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    "    max_concurrent=2000,\n",
    "    num_samples=1,\n",
    "    cache=True,\n",
    ")\n",
    "# Evaluate olympiads accuracy across all RL checkpoints (on val split)\n",
    "sandbag_accuracies, sandbag_eval_results = await run_olympiads_evaluation(\n",
    "    service_client=service_client,\n",
    "    paths=sampling_paths,\n",
    "    system_prompt=sandbag_system_prompt,\n",
    "    olympiads_prompt=OLYMPIADS_PROMPT,\n",
    "    config=generate_config,\n",
    "    num_problems=100,\n",
    "    save=True,\n",
    "    save_dir=save_dir / 'sandbag',\n",
    "    save_prefix='sandbag',\n",
    "    split='val',\n",
    ")\n",
    "\n",
    "# Evaluate olympiads accuracy across all RL checkpoints (on val split)\n",
    "benign_accuracies, benign_eval_results = await run_olympiads_evaluation(\n",
    "    service_client=service_client,\n",
    "    paths=sampling_paths,\n",
    "    system_prompt=benign_system_prompt,\n",
    "    olympiads_prompt=OLYMPIADS_PROMPT,\n",
    "    config=generate_config,\n",
    "    num_problems=100,\n",
    "    save=True,\n",
    "    save_dir=save_dir / 'benign',\n",
    "    save_prefix='benign',\n",
    "    split='val',\n",
    ")\n",
    "print(f'Sandbag Accuracies: {sandbag_accuracies}')\n",
    "print(f'Benign Accuracies: {benign_accuracies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_rl_eval_curves(benign_dir, sandbag_dir, title):\n",
    "    \"\"\"Plot benign accuracy and sandbag accuracy across RL epochs.\n",
    "    \n",
    "    Discovers epoch numbers from filenames matching *epoch_{N}_final.json in each dir.\n",
    "    \"\"\"\n",
    "    benign_dir = Path(benign_dir)\n",
    "    sandbag_dir = Path(sandbag_dir)\n",
    "    epoch_re = re.compile(r'epoch_(\\d+)_final\\.json$')\n",
    "\n",
    "    def load_dir(d):\n",
    "        \"\"\"Return {epoch: [correct_bools]} sorted by epoch.\"\"\"\n",
    "        results = {}\n",
    "        for f in sorted(d.glob('*epoch_*_final.json')):\n",
    "            m = epoch_re.search(f.name)\n",
    "            if m:\n",
    "                epoch = int(m.group(1))\n",
    "                with open(f) as fh:\n",
    "                    results[epoch] = [r['correct'] for r in json.load(fh)]\n",
    "        return dict(sorted(results.items()))\n",
    "\n",
    "    benign_data = load_dir(benign_dir)\n",
    "    sandbag_data = load_dir(sandbag_dir)\n",
    "\n",
    "    epochs = sorted(benign_data.keys() & sandbag_data.keys())\n",
    "\n",
    "    def mean_and_ci(vals_per_epoch):\n",
    "        means, lows, highs = [], [], []\n",
    "        for vals in vals_per_epoch:\n",
    "            arr = np.array(vals, dtype=float)\n",
    "            mean = arr.mean()\n",
    "            se = arr.std(ddof=1) / np.sqrt(len(arr))\n",
    "            means.append(mean)\n",
    "            lows.append(mean - 1.96 * se)\n",
    "            highs.append(mean + 1.96 * se)\n",
    "        return means, lows, highs\n",
    "\n",
    "    benign_mean, benign_lo, benign_hi = mean_and_ci([benign_data[e] for e in epochs])\n",
    "    sandbag_mean, sandbag_lo, sandbag_hi = mean_and_ci([sandbag_data[e] for e in epochs])\n",
    "\n",
    "    plt.plot(epochs, benign_mean, marker='o', color='blue', label='Benign Accuracy')\n",
    "    plt.fill_between(epochs, benign_lo, benign_hi, color='blue', alpha=0.15)\n",
    "\n",
    "    plt.plot(epochs, sandbag_mean, marker='s', color='red', label='Sandbag Accuracy')\n",
    "    plt.fill_between(epochs, sandbag_lo, sandbag_hi, color='red', alpha=0.15)\n",
    "\n",
    "    plt.axhline(y = 0.64, color = 'black', linestyle = '--', label = 'Benign Performance')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Olympiads Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rl_eval_curves(\n",
    "    benign_dir='/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/malign_inits/verbalizing_plpd/rl_random_apps_mix_bsz_64/benign',\n",
    "    sandbag_dir='/Users/spra/Desktop/Astra 2026/qwen-prompt-distillation/malign_inits/verbalizing_plpd/rl_random_apps_mix_bsz_64/sandbag',\n",
    "    title = 'RLRR On Verbalizing PLPD, 75% APPS'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
